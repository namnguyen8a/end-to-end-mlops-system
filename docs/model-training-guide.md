# ðŸ“Š DETAILED ML MODEL SPECIFICATIONS & TRAINING GUIDE

## Part 1: Feature Engineering Pipeline (Production Code)

### Feature Computation Module
```python
# src/features/technical_indicators.py

import pandas as pd
import numpy as np
from typing import Tuple, List
from talib import abstract
from sklearn.preprocessing import StandardScaler

class WeeklyIndicatorCalculator:
    """Compute 20 weekly technical indicators from daily OHLCV"""
    
    def __init__(self):
        self.scaler = StandardScaler()
    
    def compute_all_features(self, daily_df: pd.DataFrame) -> pd.DataFrame:
        """
        Input: daily_df with columns [date, open, high, low, close, volume]
        Output: weekly_features with 20 indicators
        """
        
        # Resample to weekly (Friday close)
        weekly = daily_df.set_index('date').resample('W-FRI').agg({
            'open': 'first',
            'high': 'max',
            'low': 'min',
            'close': 'last',
            'volume': 'sum',
        }).dropna()
        
        # Compute features
        features = pd.DataFrame(index=weekly.index)
        
        # 1. Momentum (ret_1w, ret_4w, ret_12w, ret_26w, ret_52w)
        features['ret_1w'] = weekly['close'].pct_change(1)
        features['ret_4w'] = weekly['close'].pct_change(4)
        features['ret_12w'] = weekly['close'].pct_change(12)
        features['ret_26w'] = weekly['close'].pct_change(26)
        features['ret_52w'] = weekly['close'].pct_change(52)
        
        # 2. Trend (MA crosses, price vs MA, breakout)
        features['MA10_w'] = weekly['close'].rolling(10).mean()
        features['MA20_w'] = weekly['close'].rolling(20).mean()
        features['MA40_w'] = weekly['close'].rolling(40).mean()
        features['price_vs_MA20_w'] = weekly['close'] / features['MA20_w']
        features['MA_cross_10_40_w'] = (features['MA10_w'] > features['MA40_w']).astype(int)
        
        highest_high_20w = weekly['high'].rolling(20).max()
        features['breakout_20w'] = weekly['close'] / highest_high_20w
        
        # 3. Volatility (vol_4w, ATR, MAX returns)
        daily_returns = daily_df['close'].pct_change()
        features['vol_4w'] = daily_returns.rolling(20).std()
        
        # ATR calculation
        tr = pd.DataFrame({
            'tr1': weekly['high'] - weekly['low'],
            'tr2': abs(weekly['high'] - weekly['close'].shift(1)),
            'tr3': abs(weekly['low'] - weekly['close'].shift(1)),
        })
        tr_max = tr.max(axis=1)
        atr_4w = tr_max.rolling(4).mean()
        features['ATR_4w_pct'] = atr_4w / weekly['close']
        
        # Extreme moves
        features['MAX_1w'] = daily_returns.rolling(5).apply(lambda x: abs(x).max())
        features['MAX_4w'] = daily_returns.rolling(20).apply(lambda x: abs(x).max())
        
        # Bollinger Band width
        bb_sma = weekly['close'].rolling(20).mean()
        bb_std = weekly['close'].rolling(20).std()
        features['bb_width_w'] = (2 * bb_std) / weekly['close']
        
        # 4. Volume indicators
        features['vol_w'] = weekly['volume']
        features['vol_ma_4w'] = weekly['volume'].rolling(4).mean()
        features['vol_ma_12w'] = weekly['volume'].rolling(12).mean()
        
        vol_zscore = (weekly['volume'] - features['vol_ma_12w']) / weekly['volume'].rolling(12).std()
        features['vol_zscore_12w'] = vol_zscore
        
        # 5. Turnover (approximate using volume * price)
        value_traded = weekly['close'] * weekly['volume']
        features['turnover_w'] = value_traded / (weekly['close'] ** 2)
        
        # 6. OBV momentum
        obv = (np.sign(daily_df['close'].diff()) * daily_df['volume']).cumsum()
        obv_weekly = obv.resample('W-FRI').last()
        features['obv_mom_4w'] = obv_weekly.diff(4)
        
        # 7. Illiquidity (Amihud)
        amihud = abs(daily_returns) / (daily_df['close'] * daily_df['volume'])
        features['amihud_w'] = amihud.rolling(5).mean().resample('W-FRI').last()
        features['illiq_rank'] = features['amihud_w'].rank(pct=True)
        
        # 8. Oscillators (RSI, MACD)
        close_arr = np.array(weekly['close'])
        features['RSI_14w'] = abstract.RSI(close_arr, timeperiod=14)
        features['RSI_speed'] = features['RSI_14w'].diff()
        
        macd, signal, hist = abstract.MACD(close_arr, fastperiod=12, slowperiod=26, signalperiod=9)
        features['MACD_hist_w'] = hist
        
        # Handle NaN values
        features = features.fillna(method='bfill').fillna(method='ffill')
        
        # Winsorize extreme values
        features = self._winsorize_features(features)
        
        return features
    
    def _winsorize_features(self, df: pd.DataFrame, limits=(0.01, 0.99)) -> pd.DataFrame:
        """Winsorize at 1st and 99th percentile to handle outliers"""
        for col in df.columns:
            lower = df[col].quantile(limits[0])
            upper = df[col].quantile(limits[1])
            df[col] = df[col].clip(lower, upper)
        return df
    
    def normalize_features(self, X_train: pd.DataFrame, X_test: pd.DataFrame = None):
        """Normalize using training data statistics"""
        X_train_scaled = self.scaler.fit_transform(X_train)
        
        if X_test is not None:
            X_test_scaled = self.scaler.transform(X_test)
            return pd.DataFrame(X_train_scaled, index=X_train.index, columns=X_train.columns), \
                   pd.DataFrame(X_test_scaled, index=X_test.index, columns=X_test.columns)
        
        return pd.DataFrame(X_train_scaled, index=X_train.index, columns=X_train.columns)


# Usage in training pipeline
calculator = WeeklyIndicatorCalculator()

# Load 3 years of daily data
daily_data = pd.read_csv('vnm_daily.csv')  # date, open, high, low, close, volume

# Compute features
features = calculator.compute_all_features(daily_data)

# Generate labels: next month's price target
next_month_close = features['close'].shift(-4)  # 4 weeks ahead
price_target = next_month_close

# Pattern label: UP (>1%), DOWN (<-1%), SIDEWAYS
ret_next_month = (next_month_close - features['close']) / features['close']
pattern = pd.cut(ret_next_month, bins=[-np.inf, -0.01, 0.01, np.inf], 
                 labels=['DOWN', 'SIDEWAYS', 'UP'])

# Split train/test (temporal)
train_size = int(len(features) * 0.8)
X_train = features.iloc[:train_size]
X_test = features.iloc[train_size:]

y_price_train = price_target.iloc[:train_size]
y_price_test = price_target.iloc[train_size:]

y_pattern_train = pattern.iloc[:train_size]
y_pattern_test = pattern.iloc[train_size:]

print(f"âœ… Features computed: {features.shape}")
print(f"   Train: {X_train.shape}, Test: {X_test.shape}")
```

---

## Part 2: Linear Regression Model Training

```python
# src/models/price_model.py

import pandas as pd
import numpy as np
from sklearn.preprocessing import StandardScaler, PolynomialFeatures
from sklearn.pipeline import Pipeline
from sklearn.linear_model import Ridge
from sklearn.metrics import mean_absolute_error, r2_score, mean_squared_error
import mlflow
import mlflow.sklearn

class PriceTargetModel:
    """Predict next month's closing price using Linear Regression"""
    
    def __init__(self, degree=2, alpha=1.0):
        self.degree = degree
        self.alpha = alpha
        self.model = None
    
    def build(self):
        """Build sklearn pipeline"""
        self.model = Pipeline([
            ('scaler', StandardScaler()),
            ('poly_features', PolynomialFeatures(degree=self.degree, include_bias=False)),
            ('regressor', Ridge(alpha=self.alpha, solver='auto')),
        ])
        return self.model
    
    def train(self, X_train: pd.DataFrame, y_train: pd.Series):
        """Train the model"""
        self.model.fit(X_train, y_train)
        return self
    
    def predict(self, X):
        """Make predictions"""
        return self.model.predict(X)
    
    def evaluate(self, X_test: pd.DataFrame, y_test: pd.Series) -> dict:
        """Evaluate on test set"""
        y_pred = self.predict(X_test)
        
        mae = mean_absolute_error(y_test, y_pred)
        rmse = np.sqrt(mean_squared_error(y_test, y_pred))
        r2 = r2_score(y_test, y_pred)
        mape = np.mean(np.abs((y_test - y_pred) / y_test)) * 100
        
        metrics = {
            'MAE': mae,
            'RMSE': rmse,
            'R2': r2,
            'MAPE': mape,
        }
        
        return metrics
    
    def train_with_mlflow(self, X_train, y_train, X_test, y_test, 
                         experiment_name="price_target_model"):
        """Train and log to MLflow"""
        
        with mlflow.start_run(experiment_id=experiment_name):
            # Log parameters
            mlflow.log_params({
                'model_type': 'Linear Regression',
                'poly_degree': self.degree,
                'ridge_alpha': self.alpha,
                'train_size': len(X_train),
                'test_size': len(X_test),
            })
            
            # Train
            self.build()
            self.train(X_train, y_train)
            
            # Evaluate
            train_metrics = self.evaluate(X_train, y_train)
            test_metrics = self.evaluate(X_test, y_test)
            
            # Log metrics
            for key, value in train_metrics.items():
                mlflow.log_metric(f'train_{key}', value)
            
            for key, value in test_metrics.items():
                mlflow.log_metric(f'test_{key}', value)
            
            # Log model
            mlflow.sklearn.log_model(self.model, "model")
            
            print(f"âœ… Training completed!")
            print(f"   Test MAE: {test_metrics['MAE']:.4f}")
            print(f"   Test R2: {test_metrics['R2']:.4f}")
            
            return test_metrics


# Usage
if __name__ == "__main__":
    # Load data from pipeline
    features = pd.read_csv('features_train.csv', index_col=0)
    labels = pd.read_csv('labels_train.csv', index_col=0)
    
    # Split
    train_size = int(len(features) * 0.8)
    X_train, X_test = features.iloc[:train_size], features.iloc[train_size:]
    y_train, y_test = labels['price_target'].iloc[:train_size], labels['price_target'].iloc[train_size:]
    
    # Train with MLflow
    mlflow.set_experiment("price_target_model")
    model = PriceTargetModel(degree=2, alpha=1.0)
    metrics = model.train_with_mlflow(X_train, y_train, X_test, y_test)
```

---

## Part 3: LSTM Pattern Classification Model

```python
# src/models/pattern_model.py

import pandas as pd
import numpy as np
import tensorflow as tf
from tensorflow import keras
from tensorflow.keras import layers
import mlflow
import mlflow.tensorflow
from sklearn.preprocessing import StandardScaler
from sklearn.metrics import classification_report, confusion_matrix, accuracy_score

class PatternLSTMModel:
    """Classify monthly price movement pattern: UP, DOWN, SIDEWAYS"""
    
    def __init__(self, seq_len=8, n_features=20, units=[64, 32]):
        self.seq_len = seq_len
        self.n_features = n_features
        self.units = units
        self.model = None
        self.scaler = StandardScaler()
    
    def build(self):
        """Build LSTM model with stacked layers"""
        
        self.model = keras.Sequential([
            # Input layer + LSTM 1
            layers.LSTM(
                self.units[0],
                activation='relu',
                return_sequences=True,
                input_shape=(self.seq_len, self.n_features),
                name='lstm_1'
            ),
            layers.Dropout(0.2),
            
            # LSTM 2
            layers.LSTM(
                self.units[1],
                activation='relu',
                return_sequences=False,
                name='lstm_2'
            ),
            layers.Dropout(0.2),
            
            # Dense layers
            layers.Dense(16, activation='relu', name='dense_1'),
            layers.Dense(3, activation='softmax', name='output'),  # UP, DOWN, SIDEWAYS
        ])
        
        self.model.compile(
            optimizer=keras.optimizers.Adam(learning_rate=0.001),
            loss='categorical_crossentropy',
            metrics=[
                'accuracy',
                keras.metrics.Precision(name='precision'),
                keras.metrics.Recall(name='recall'),
            ]
        )
        
        print(self.model.summary())
        return self.model
    
    def prepare_sequences(self, X: pd.DataFrame) -> np.ndarray:
        """
        Convert dataframe to sequence format for LSTM
        Input: (n_weeks, 20 features)
        Output: (n_samples, seq_len=8, 20 features)
        """
        sequences = []
        for i in range(len(X) - self.seq_len + 1):
            sequences.append(X.iloc[i:i + self.seq_len].values)
        
        return np.array(sequences)
    
    def train(self, X_train_seq: np.ndarray, y_train: np.ndarray,
              X_val_seq: np.ndarray, y_val: np.ndarray,
              epochs=50, batch_size=32):
        """Train the LSTM model"""
        
        history = self.model.fit(
            X_train_seq, y_train,
            validation_data=(X_val_seq, y_val),
            epochs=epochs,
            batch_size=batch_size,
            callbacks=[
                keras.callbacks.EarlyStopping(
                    monitor='val_loss',
                    patience=5,
                    restore_best_weights=True,
                    verbose=1
                ),
                keras.callbacks.ReduceLROnPlateau(
                    monitor='val_loss',
                    factor=0.5,
                    patience=3,
                    min_lr=0.00001,
                    verbose=1
                ),
            ],
            verbose=1
        )
        
        return history
    
    def evaluate(self, X_test_seq: np.ndarray, y_test: np.ndarray) -> dict:
        """Evaluate on test set"""
        
        y_pred_probs = self.model.predict(X_test_seq)
        y_pred = np.argmax(y_pred_probs, axis=1)
        y_true = np.argmax(y_test, axis=1)
        
        accuracy = accuracy_score(y_true, y_pred)
        
        # Classification report
        class_names = ['DOWN', 'SIDEWAYS', 'UP']
        report = classification_report(y_true, y_pred, target_names=class_names, output_dict=True)
        
        # Confusion matrix
        cm = confusion_matrix(y_true, y_pred)
        
        metrics = {
            'accuracy': accuracy,
            'precision_up': report['UP']['precision'],
            'recall_up': report['UP']['recall'],
            'f1_up': report['UP']['f1-score'],
            'precision_down': report['DOWN']['precision'],
            'recall_down': report['DOWN']['recall'],
            'f1_down': report['DOWN']['f1-score'],
        }
        
        return metrics, report, cm
    
    def train_with_mlflow(self, X_train_seq, y_train, X_val_seq, y_val,
                         X_test_seq, y_test, experiment_name="pattern_lstm"):
        """Train and log to MLflow"""
        
        with mlflow.start_run(experiment_id=experiment_name):
            # Log parameters
            mlflow.log_params({
                'model_type': 'LSTM Pattern Classifier',
                'sequence_length': self.seq_len,
                'n_features': self.n_features,
                'lstm_units': str(self.units),
                'train_samples': len(X_train_seq),
                'val_samples': len(X_val_seq),
                'test_samples': len(X_test_seq),
            })
            
            # Build and train
            self.build()
            history = self.train(X_train_seq, y_train, X_val_seq, y_val, epochs=50)
            
            # Evaluate
            metrics, report, cm = self.evaluate(X_test_seq, y_test)
            
            # Log metrics
            for key, value in metrics.items():
                mlflow.log_metric(key, value)
            
            # Log model
            mlflow.tensorflow.log_model(self.model, "model")
            
            print(f"âœ… LSTM Training completed!")
            print(f"   Test Accuracy: {metrics['accuracy']:.4f}")
            print(f"   Test F1 (UP): {metrics['f1_up']:.4f}")
            
            return metrics, history


# Usage in training notebook
if __name__ == "__main__":
    # Load features
    features = pd.read_csv('features_train.csv', index_col=0)
    labels = pd.read_csv('labels_train.csv', index_col=0)
    
    # Split
    train_size = int(len(features) * 0.7)
    val_size = int(len(features) * 0.85)
    
    X_train = features.iloc[:train_size]
    X_val = features.iloc[train_size:val_size]
    X_test = features.iloc[val_size:]
    
    y_pattern = keras.utils.to_categorical(
        pd.factorize(labels['pattern'])[0],
        num_classes=3
    )
    y_train = y_pattern[:train_size]
    y_val = y_pattern[train_size:val_size]
    y_test = y_pattern[val_size:]
    
    # Prepare sequences
    model = PatternLSTMModel(seq_len=8, n_features=20)
    X_train_seq = model.prepare_sequences(X_train)
    X_val_seq = model.prepare_sequences(X_val)
    X_test_seq = model.prepare_sequences(X_test)
    
    # Train with MLflow
    mlflow.set_experiment("pattern_lstm_classifier")
    metrics, history = model.train_with_mlflow(
        X_train_seq, y_train,
        X_val_seq, y_val,
        X_test_seq, y_test
    )
```

---

## Part 4: Model Serialization & MLflow Registry

```python
# src/models/registry.py

import mlflow
import mlflow.sklearn
import mlflow.tensorflow
import pickle
from pathlib import Path

class ModelRegistry:
    """Manage model versions and promotion through MLflow"""
    
    def __init__(self, tracking_uri="http://localhost:5000"):
        mlflow.set_tracking_uri(tracking_uri)
    
    def register_price_model(self, model, run_id: str, test_mae: float):
        """Register price prediction model"""
        
        model_uri = f"runs:/{run_id}/model"
        
        registered_model = mlflow.register_model(
            model_uri=model_uri,
            name="price_target_model"
        )
        
        print(f"âœ… Registered: {registered_model.name} v{registered_model.version}")
        
        # Add metadata
        client = mlflow.tracking.MlflowClient()
        client.update_model_version(
            name="price_target_model",
            version=registered_model.version,
            description=f"Price target model, Test MAE: {test_mae:.4f}",
        )
        
        return registered_model
    
    def promote_to_staging(self, model_name: str, version: int):
        """Promote model to Staging stage"""
        client = mlflow.tracking.MlflowClient()
        client.transition_model_version_stage(
            name=model_name,
            version=version,
            stage="Staging"
        )
        print(f"âœ… {model_name} v{version} â†’ Staging")
    
    def promote_to_production(self, model_name: str, version: int):
        """Promote model to Production stage"""
        client = mlflow.tracking.MlflowClient()
        
        # Archive current production model
        prod_models = client.get_latest_versions(model_name, stages=["Production"])
        if prod_models:
            old_version = prod_models[0].version
            client.transition_model_version_stage(
                name=model_name,
                version=old_version,
                stage="Archived"
            )
            print(f"   {model_name} v{old_version} â†’ Archived")
        
        # Promote new model
        client.transition_model_version_stage(
            name=model_name,
            version=version,
            stage="Production"
        )
        print(f"âœ… {model_name} v{version} â†’ Production")
    
    def get_production_model(self, model_name: str):
        """Load production model"""
        model_uri = f"models:/{model_name}/Production"
        model = mlflow.pyfunc.load_model(model_uri)
        return model
    
    def compare_models(self, model_name: str):
        """Compare all registered versions"""
        client = mlflow.tracking.MlflowClient()
        versions = client.get_latest_versions(model_name)
        
        print(f"\nðŸ“Š {model_name} - All Versions:")
        for version in versions:
            print(f"   v{version.version} [{version.current_stage}] - "
                  f"Last modified: {version.last_updated_timestamp}")
```

---

## Part 5: Complete Training Workflow (Colab)

```python
# training_notebook.py (Run in Google Colab)

# ! pip install vnstock mlflow scikit-learn tensorflow keras talib pandas numpy

import pandas as pd
import numpy as np
from vnstock import Quote
import mlflow
import mlflow.sklearn
import mlflow.tensorflow

# ============ STEP 1: Download Data ============

symbols = ['VNM', 'VCB', 'ACB', 'VIC', 'TCB']  # Example stocks
all_data = {}

for symbol in symbols:
    print(f"Fetching {symbol}...")
    quote = Quote(symbol=symbol)
    data = quote.history(start='2022-01-01', end='2025-11-20')
    all_data[symbol] = data

print(f"âœ… Downloaded {len(all_data)} stocks")

# ============ STEP 2: Feature Engineering ============

from src.features.technical_indicators import WeeklyIndicatorCalculator

calculator = WeeklyIndicatorCalculator()
all_features = {}
all_labels = {}

for symbol, daily_data in all_data.items():
    print(f"Computing features for {symbol}...")
    features = calculator.compute_all_features(daily_data)
    
    # Generate labels
    next_month_close = features['close'].shift(-4)
    ret_next_month = (next_month_close - features['close']) / features['close']
    pattern = pd.cut(ret_next_month, bins=[-np.inf, -0.01, 0.01, np.inf],
                     labels=['DOWN', 'SIDEWAYS', 'UP'])
    
    all_features[symbol] = features
    all_labels[symbol] = {'price_target': next_month_close, 'pattern': pattern}

print(f"âœ… Features computed for {len(all_features)} stocks")

# ============ STEP 3: Combine All Data ============

X_combined = pd.concat(all_features.values(), axis=0).dropna()
y_price_combined = pd.concat([v['price_target'] for v in all_labels.values()], axis=0).loc[X_combined.index]
y_pattern_combined = pd.concat([v['pattern'] for v in all_labels.values()], axis=0).loc[X_combined.index]

# Train/test split (80/20 temporal)
train_size = int(len(X_combined) * 0.8)
X_train = X_combined.iloc[:train_size]
X_test = X_combined.iloc[train_size:]

y_price_train = y_price_combined.iloc[:train_size]
y_price_test = y_price_combined.iloc[train_size:]

y_pattern_train = y_pattern_combined.iloc[:train_size]
y_pattern_test = y_pattern_combined.iloc[train_size:]

print(f"âœ… Combined dataset: {X_combined.shape}")
print(f"   Train: {X_train.shape}, Test: {X_test.shape}")

# ============ STEP 4: Train Price Model ============

from src.models.price_model import PriceTargetModel

mlflow.set_experiment("price_target_model_final")

model_price = PriceTargetModel(degree=2, alpha=1.0)
price_metrics = model_price.train_with_mlflow(
    X_train, y_price_train,
    X_test, y_price_test
)

# ============ STEP 5: Train Pattern Model ============

from src.models.pattern_model import PatternLSTMModel

mlflow.set_experiment("pattern_lstm_final")

model_lstm = PatternLSTMModel(seq_len=8, n_features=20)
X_train_seq = model_lstm.prepare_sequences(X_train)
X_test_seq = model_lstm.prepare_sequences(X_test)

y_pattern_train_cat = keras.utils.to_categorical(
    pd.factorize(y_pattern_train)[0], num_classes=3
)
y_pattern_test_cat = keras.utils.to_categorical(
    pd.factorize(y_pattern_test)[0], num_classes=3
)

pattern_metrics, _ = model_lstm.train_with_mlflow(
    X_train_seq, y_pattern_train_cat,
    X_test_seq, y_pattern_test_cat,
    X_test_seq, y_pattern_test_cat
)

print(f"\nâœ… Training Complete!")
print(f"   Price Model MAE: {price_metrics['test_MAE']:.4f}")
print(f"   Pattern Model Accuracy: {pattern_metrics['accuracy']:.4f}")
```

---

## ðŸ“‹ TRAINING CHECKLIST

- [ ] Clone repository & install dependencies
- [ ] Set up MLflow tracking server locally
- [ ] Download 3 years of historical data (vnstock API)
- [ ] Run feature engineering pipeline (20 indicators)
- [ ] Verify feature statistics (no NaN, proper normalization)
- [ ] Train linear regression model (validate MAE < 2.5%)
- [ ] Train LSTM pattern model (validate accuracy > 70%)
- [ ] Register both models in MLflow
- [ ] Test inference latency (<100ms per prediction)
- [ ] Backtest on historical data (Sharpe > 0.5)
- [ ] Deploy to FastAPI server
- [ ] Test chatbot integration
- [ ] Set up monitoring alerts
- [ ] Deploy to production (Docker Compose)

---

**Version:** 1.0.0  
**Status:** Production Ready âœ…
